{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-17T18:25:23.545278Z","iopub.execute_input":"2021-11-17T18:25:23.545662Z","iopub.status.idle":"2021-11-17T18:25:23.584887Z","shell.execute_reply.started":"2021-11-17T18:25:23.545570Z","shell.execute_reply":"2021-11-17T18:25:23.583649Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.models import load_model\nfrom tensorflow.keras.utils  import to_categorical\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:26.240351Z","iopub.execute_input":"2021-11-17T18:25:26.240772Z","iopub.status.idle":"2021-11-17T18:25:32.786125Z","shell.execute_reply.started":"2021-11-17T18:25:26.240732Z","shell.execute_reply":"2021-11-17T18:25:32.785136Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mobile-health/mhealth_raw_data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:32.788492Z","iopub.execute_input":"2021-11-17T18:25:32.788850Z","iopub.status.idle":"2021-11-17T18:25:37.163115Z","shell.execute_reply.started":"2021-11-17T18:25:32.788762Z","shell.execute_reply":"2021-11-17T18:25:37.162101Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:37.170974Z","iopub.execute_input":"2021-11-17T18:25:37.173501Z","iopub.status.idle":"2021-11-17T18:25:37.362535Z","shell.execute_reply.started":"2021-11-17T18:25:37.173459Z","shell.execute_reply":"2021-11-17T18:25:37.361478Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df['Activity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:37.364705Z","iopub.execute_input":"2021-11-17T18:25:37.365747Z","iopub.status.idle":"2021-11-17T18:25:37.383129Z","shell.execute_reply.started":"2021-11-17T18:25:37.365682Z","shell.execute_reply":"2021-11-17T18:25:37.381794Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"activity_labels = \n    0: \"None\",\n    1: \"Standing still (1 min)\",\n    2: \"Sitting and relaxing (1 min)\",\n    3: \"Lying down (1 min)\",\n    4: \"Walking (1 min)\",\n    5: \"Climbing stairs (1 min)\",\n    6: \"Waist bends forward (20x)\",\n    7: \"Frontal elevation of arms (20x)\",\n    8: \"Knees bending (crouching) (20x)\",\n    9: \"Cycling (1 min)\",\n    10: \"Jogging (1 min)\",\n    11: \"Running (1 min)\",\n    12: \"Jump front & back (20x)\"","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:24:33.386281Z","iopub.execute_input":"2021-11-12T10:24:33.386624Z","iopub.status.idle":"2021-11-12T10:24:33.396444Z","shell.execute_reply.started":"2021-11-12T10:24:33.386592Z","shell.execute_reply":"2021-11-12T10:24:33.395411Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df = df[(df.Activity != 0) & (df.Activity != 2) & (df.Activity != 3)]\ndf.Activity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:38.685500Z","iopub.execute_input":"2021-11-17T18:25:38.685976Z","iopub.status.idle":"2021-11-17T18:25:38.733454Z","shell.execute_reply.started":"2021-11-17T18:25:38.685940Z","shell.execute_reply":"2021-11-17T18:25:38.732297Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.subject.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:42.798708Z","iopub.execute_input":"2021-11-17T18:25:42.799370Z","iopub.status.idle":"2021-11-17T18:25:42.848809Z","shell.execute_reply.started":"2021-11-17T18:25:42.799333Z","shell.execute_reply":"2021-11-17T18:25:42.846924Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# creating equal group sizes \ndf = df.groupby(['subject', 'Activity']).apply(lambda grp: grp.sample(n=1000))","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:48.790851Z","iopub.execute_input":"2021-11-17T18:25:48.791167Z","iopub.status.idle":"2021-11-17T18:25:49.165909Z","shell.execute_reply.started":"2021-11-17T18:25:48.791135Z","shell.execute_reply":"2021-11-17T18:25:49.164722Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:51.082102Z","iopub.execute_input":"2021-11-17T18:25:51.082540Z","iopub.status.idle":"2021-11-17T18:25:51.088285Z","shell.execute_reply.started":"2021-11-17T18:25:51.082507Z","shell.execute_reply":"2021-11-17T18:25:51.086741Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:53.518303Z","iopub.execute_input":"2021-11-17T18:25:53.518589Z","iopub.status.idle":"2021-11-17T18:25:53.544226Z","shell.execute_reply.started":"2021-11-17T18:25:53.518557Z","shell.execute_reply":"2021-11-17T18:25:53.542944Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.Activity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:25:56.433410Z","iopub.execute_input":"2021-11-17T18:25:56.433696Z","iopub.status.idle":"2021-11-17T18:25:56.444297Z","shell.execute_reply.started":"2021-11-17T18:25:56.433663Z","shell.execute_reply":"2021-11-17T18:25:56.442922Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns=['subject'])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:01.037338Z","iopub.execute_input":"2021-11-17T18:26:01.037679Z","iopub.status.idle":"2021-11-17T18:26:01.048386Z","shell.execute_reply.started":"2021-11-17T18:26:01.037646Z","shell.execute_reply":"2021-11-17T18:26:01.047374Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(df)*0.7)\ntrain, test = df.iloc[:train_size],df.iloc[train_size:]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:03.359484Z","iopub.execute_input":"2021-11-17T18:26:03.359990Z","iopub.status.idle":"2021-11-17T18:26:03.365227Z","shell.execute_reply.started":"2021-11-17T18:26:03.359956Z","shell.execute_reply":"2021-11-17T18:26:03.364198Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train = train.drop('Activity', axis = 1)\ny_train = train.loc[:,['Activity']]\n\n# Split test data to X and y\nX_test = test.drop('Activity', axis = 1)\ny_test = test.loc[:,['Activity']]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:05.444535Z","iopub.execute_input":"2021-11-17T18:26:05.445458Z","iopub.status.idle":"2021-11-17T18:26:05.459180Z","shell.execute_reply.started":"2021-11-17T18:26:05.445394Z","shell.execute_reply":"2021-11-17T18:26:05.457982Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.utils  import to_categorical\n\n# Different scaler for input and output\nscaler_x = MinMaxScaler(feature_range = (0,1))\n\n# Fit the scaler using available training data\ninput_scaler = scaler_x.fit(X_train)\n\n# Apply the scaler to training data\ntrain_x_norm = input_scaler.transform(X_train)\n\n# Apply the scaler to test data\ntest_x_norm = input_scaler.transform(X_test)\n\n# one-hot encode y\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:07.999512Z","iopub.execute_input":"2021-11-17T18:26:08.000021Z","iopub.status.idle":"2021-11-17T18:26:08.021684Z","shell.execute_reply.started":"2021-11-17T18:26:07.999987Z","shell.execute_reply":"2021-11-17T18:26:08.020779Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def threeD_dataset (X, y, time_steps = 1):\n    Xs, ys = [], []\n    \n    for i in range(len(X)-time_steps):\n        v = X[i:i+time_steps, :]\n        Xs.append(v)\n        ys.append(y[i+time_steps])\n        \n    return np.array(Xs), np.array(ys)\n\n\nTIME_STEPS = 50\n\nX_test, y_test = threeD_dataset(test_x_norm, y_test, TIME_STEPS)\nX_train, y_train = threeD_dataset(train_x_norm, y_train, TIME_STEPS)\n\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape) \nprint('y_test.shape: ', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:10.673301Z","iopub.execute_input":"2021-11-17T18:26:10.673599Z","iopub.status.idle":"2021-11-17T18:26:11.120495Z","shell.execute_reply.started":"2021-11-17T18:26:10.673569Z","shell.execute_reply":"2021-11-17T18:26:11.119462Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# fit and evaluate a model\ndef fit_evaluate_model(X_train, y_train, X_test, y_test, n_filters):\n    verbose, epochs, batch_size = 0, 10, 32\n    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n    # create model\n    model = Sequential()\n    model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n    model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    plot_model(model, show_shapes=True, to_file='sequential.png')\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    # evaluate model\n    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n    return accuracy\n \n# summarize performance (accuracy, mean, sd)\ndef performance_summary(scores, params):\n    print(scores, params)\n    for score in range(len(scores)):\n        m, s = mean(scores[score]), std(scores[score])\n        print('Filter=%d: %.3f%% (+/-%.3f)' % (params[score], m, s))\n    # boxplot of scores\n    plt.boxplot(scores, labels=params)\n    plt.savefig('cnn_filters.png')\n \n# run the trials\ndef run_trials(params, repeats=5):\n    # test each parameter\n    all_scores = list()\n    for param in params:\n        scores = list()\n        for repeat in range(repeats):\n            score = fit_evaluate_model(X_train, y_train, X_test, y_test, param)\n            score = score * 100.0\n            print('>Filter=%d #%d: %.3f' % (param, repeat+1, score))\n            scores.append(score)\n        all_scores.append(scores)\n    performance_summary(all_scores, params)\n \n# run \nn_filters = [8, 16, 32, 64, 128, 256]\nrun_trials(n_filters)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T18:26:15.541175Z","iopub.execute_input":"2021-11-17T18:26:15.541837Z","iopub.status.idle":"2021-11-17T19:09:35.769914Z","shell.execute_reply.started":"2021-11-17T18:26:15.541782Z","shell.execute_reply":"2021-11-17T19:09:35.768810Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# fit and evaluate a model\ndef fit_evaluate_model(X_train, y_train, X_test, y_test, n_kernel):\n    verbose, epochs, batch_size = 0, 10, 32\n    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n    # create model\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=n_kernel, activation='relu', input_shape=(n_timesteps,n_features)))\n    model.add(Conv1D(filters=64, kernel_size=n_kernel, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    # evaluate model\n    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n    return accuracy\n \n# summarize performance (accuracy, mean, sd)\ndef performance_summary(scores, params):\n    print(scores, params)\n    for score in range(len(scores)):\n        m, s = mean(scores[score]), std(scores[score])\n        print('Kernel=%d: %.3f%% (+/-%.3f)' % (params[score], m, s))\n    # boxplot of scores\n    plt.boxplot(scores, labels=params)\n    plt.savefig('cnn_filters.png')\n \n# run the trials\ndef run_trials(params, repeats=5):\n    # test each parameter\n    all_scores = list()\n    for param in params:\n        scores = list()\n        for repeat in range(repeats):\n            score = fit_evaluate_model(X_train, y_train, X_test, y_test, param)\n            score = score * 100.0\n            print('>Kernel=%d #%d: %.3f' % (param, repeat+1, score))\n            scores.append(score)\n        all_scores.append(scores)\n    performance_summary(all_scores, params)\n \n# run\nn_kernels = [2, 3, 5, 7, 11]\nrun_trials(n_kernels)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T19:09:35.772742Z","iopub.execute_input":"2021-11-17T19:09:35.773102Z","iopub.status.idle":"2021-11-17T19:44:27.990886Z","shell.execute_reply.started":"2021-11-17T19:09:35.773056Z","shell.execute_reply":"2021-11-17T19:44:27.989748Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# fit and evaluate a model\ndef fit_evaluate_model(X_train, y_train, X_test, y_test, n_dropout):\n    verbose, epochs, batch_size = 0, 10, 32\n    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n    # create model\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n    model.add(Dropout(n_dropout))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    # evaluate model\n    _, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n    return accuracy\n \n# summarize performance (accuracy, mean, sd)\ndef performance_summary(scores, params):\n    print(scores, params)\n    for score in range(len(scores)):\n        m, s = mean(scores[score]), std(scores[score])\n        print('Dropout=%.1f: %.3f%% (+/-%.3f)' % (params[score], m, s))\n    # boxplot of scores\n    plt.boxplot(scores, labels=params)\n    plt.savefig('cnn_filters.png')\n \n# run the trials\ndef run_trials(params, repeats=5):\n    # test each parameter\n    all_scores = list()\n    for param in params:\n        scores = list()\n        for repeat in range(repeats):\n            score = fit_evaluate_model(X_train, y_train, X_test, y_test, param)\n            score = score * 100.0\n            print('>Dropout=%.1f #%d: %.3f' % (param, repeat+1, score))\n            scores.append(score)\n        all_scores.append(scores)\n    performance_summary(all_scores, params)\n \n# run \nn_dropout = [0.1, 0.3, 0.5, 0.7, 0.9]\nrun_trials(n_dropout)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T19:44:27.993220Z","iopub.execute_input":"2021-11-17T19:44:27.993852Z","iopub.status.idle":"2021-11-17T20:18:50.222025Z","shell.execute_reply.started":"2021-11-17T19:44:27.993805Z","shell.execute_reply":"2021-11-17T20:18:50.220812Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from keras.layers.merge import concatenate\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\n# fit and evaluate a model\ndef fit_evaluate_model(X_train, y_train, X_test, y_test):\n    # build the model\n    verbose, epochs, batch_size = 0, 10, 32\n    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n    # input head 1\n    inputs1 = Input(shape=(n_timesteps,n_features))\n    conv1 = Conv1D(filters=256, kernel_size=2, activation='relu')(inputs1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # input head 2\n    inputs2 = Input(shape=(n_timesteps,n_features))\n    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu')(inputs2)\n    drop2 = Dropout(0.3)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # input head 3\n    inputs3 = Input(shape=(n_timesteps,n_features))\n    conv3 = Conv1D(filters=64, kernel_size=5, activation='relu')(inputs3)\n    drop3 = Dropout(0.1)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # concatenate\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(100, activation='relu')(merged)\n    outputs = Dense(n_outputs, activation='softmax')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # save a plot of the model\n    plot_model(model, show_shapes=True, to_file='multichannel.png')\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit network\n    model.fit([X_train,X_train,X_train], y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    # evaluate model\n    _, accuracy = model.evaluate([X_test,X_test,X_test], y_test, batch_size=batch_size, verbose=verbose)\n    return accuracy\n \n# summarize performance (accuracy, mean, sd)\ndef performance_summary(scores):\n    print(scores)\n    m, s = mean(scores), std(scores)\n    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n \n# run the trials\ndef run_trials(repeats=5):\n    # repeat experiment\n    scores = list()\n    for repeat in range(repeats):\n        score = fit_evaluate_model(X_train, y_train, X_test, y_test)\n        score = score * 100.0\n        print('>#%d: %.3f' % (repeat+1, score))\n        scores.append(score)\n    # performance summary \n    performance_summary(scores)\n \n# run \nrun_trials()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T20:18:50.224682Z","iopub.execute_input":"2021-11-17T20:18:50.225258Z","iopub.status.idle":"2021-11-17T20:28:14.577711Z","shell.execute_reply.started":"2021-11-17T20:18:50.225211Z","shell.execute_reply":"2021-11-17T20:28:14.576573Z"},"trusted":true},"execution_count":20,"outputs":[]}]}